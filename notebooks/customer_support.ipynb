{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict , Annotated\n",
    "import operator\n",
    "\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Sure, I'd be happy to explain!\\n\\nLLMs, or Large Language Models, are a type of artificial intelligence model that are designed to understand and generate human-like text. These models are used in a variety of applications, such as chatbots, virtual assistants, and language translation tools.\\n\\nOne important factor that can impact the performance of LLMs is latency, which is the time it takes for the model to process a request and return a response. Low latency is particularly important for LLMs for several reasons:\\n\\n1. User experience: When using an application powered by an LLM, such as a chatbot, users expect to receive responses quickly. If the latency is high, it can create a frustrating user experience and make the application feel slow and unresponsive.\\n2. Real-time applications: Some applications that use LLMs, such as real-time language translation tools, require low latency in order to function properly. If the latency is too high, it can result in delays and make the application unusable.\\n3. Cost: High latency can also have a financial impact, as it may require the use of more computing resources to process requests. This can increase the cost of operating the application.\\n\\nOverall, low latency is an important factor for LLMs because it can impact the user experience, the functionality of the application, and the cost of operating the system. By optimizing for low latency, developers can ensure that their LLM-powered applications are fast, responsive, and cost-effective.\" response_metadata={'token_usage': {'completion_tokens': 330, 'prompt_tokens': 27, 'total_tokens': 357, 'completion_time': 0.512146934, 'prompt_time': 0.005725858, 'queue_time': None, 'total_time': 0.517872792}, 'model_name': 'mixtral-8x7b-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'stop', 'logprobs': None} id='run-ed71aa46-9679-4700-b72b-8b37c3529ddc-0' usage_metadata={'input_tokens': 27, 'output_tokens': 330, 'total_tokens': 357}\n"
     ]
    }
   ],
   "source": [
    "client = ChatGroq(\n",
    "    temperature=0,\n",
    "    model=\"mixtral-8x7b-32768\",\n",
    ")\n",
    "\n",
    "SYSTEM = \"You are a helpful assistant.\"\n",
    "HUMAN = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", SYSTEM), (\"human\", HUMAN)])\n",
    "\n",
    "chain = prompt | client\n",
    "answer = chain.invoke({\"text\": \"Explain the importance of low latency for LLMs.\"})\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
